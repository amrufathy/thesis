# @package _global_

# to execute this experiment run:
# python run.py experiment=cycle.yaml

defaults:
  - override /trainer: minimal.yaml  # override trainer to null so it's not loaded from main config defaults...
  - override /model: cycle.yaml
  - override /datamodule: rocstories.yaml
  - override /callbacks: wandb.yaml  # choose callbacks from 'configs/callbacks/'
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

trainer:
  min_epochs: 1
  max_epochs: 20
  profiler: "simple"
  log_gpu_memory: True
  resume_from_checkpoint: null
  accumulate_grad_batches: 2  # regularize gradient updating with other models
  # batch size of compressor / batch size of cycle

model:
  direction: "dual"
  use_gumbel_softmax: False

datamodule:
  batch_size: 32
  percentage: 100  # percentage of dataset to load
  load_dataset_from_file: False

callbacks:
  model_checkpoint:
    monitor: "val/exp_bleu"
    dirpath: "checkpoints/"
    filename: "cycle-${now:%m-%d_%H-%M}-{epoch:02d}"

  early_stopping:
    monitor: "val/exp_bleu"

logger:
  wandb:
    id: null
