# @package _global_

# to execute this experiment run:
# python run.py experiment=cycle.yaml

defaults:
  - override /trainer: minimal.yaml  # override trainer to null so it's not loaded from main config defaults...
  - override /model: cycle.yaml
  - override /datamodule: rocstories.yaml
  - override /callbacks: wandb.yaml  # choose callbacks from 'configs/callbacks/'
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

trainer:
  min_epochs: 1
  max_epochs: 20
  profiler: "simple"
  log_gpu_memory: True
  accumulate_grad_batches: 64/${datamodule.batch_size}  # regularize gradient updating with other models

model:
  use_gumbel_softmax: False

datamodule:
  batch_size: 16
  percentage: 100  # percentage of dataset to load
  load_dataset_from_file: False

callbacks:
  model_checkpoint:
    dirpath: "checkpoints/"
    filename: "cycle-${now:%m-%d_%H-%M}-{epoch:02d}"
